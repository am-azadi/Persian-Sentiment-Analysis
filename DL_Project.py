# -*- coding: utf-8 -*-
"""DL_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SdeRWrjehulNwW_bZnAAK4R4kUjTr2tp
"""

import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import pipeline, XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

"""# PreProcessing"""

label2id = {'SAD': 0, 'HAPPY': 1, 'HATE': 2, 'SURPRISE': 3, 'ANGRY': 4, 'FEAR': 5, 'OTHER': 6}
id2label = {0: 'SAD', 1: 'HAPPY', 2: 'HATE', 3: 'SURPRISE', 4: 'ANGRY', 5: 'FEAR', 6: 'OTHER'}

train_path = '/content/drive/MyDrive/University/Deep Learning/Project/dataset/train.tsv'

train_df = pd.read_csv(train_path, sep='\t', header=None)
train_df.columns = ['text', 'label']

train_df

from matplotlib import pyplot as plt
import seaborn as sns
train_df.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

train_df['label'] = train_df['label'].map(label2id)
train_df

test_path = '/content/drive/MyDrive/University/Deep Learning/Project/dataset/test.tsv'

test_df = pd.read_csv(test_path, sep='\t', header=None)
test_df.columns = ['text', 'label']

test_df

from matplotlib import pyplot as plt
import seaborn as sns
test_df.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

test_df['label'] = test_df['label'].map(label2id)
test_df

print(f"train data size is {len(train_df)}")
print(f"test data size is {len(test_df)}")

class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

"""# Train models"""

def train_model(model_name, train_df, test_df, id2label, zero_shot, number_of_epochs = 3, learning_rates = [1e-5, 1e-5, 1e-5]):

  # Initialize tokenizer and model
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 7)

  # Move the model to GPU
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model.to(device)

  # Create datasets and dataloaders
  train_dataset = CustomDataset(train_df['text'].values, train_df['label'].values, tokenizer)
  test_dataset = CustomDataset(test_df['text'].values, test_df['label'].values, tokenizer)

  train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)
  test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)

  if not zero_shot:
    # Training loop (optional if you have labeled data)
    # Skip this part if you're doing zero-shot learning
    # Train the model
    for epoch in range(number_of_epochs):
        # Define optimizer and loss function
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rates[epoch])
        criterion = torch.nn.CrossEntropyLoss()
        model.train()
        for i, batch in enumerate(train_dataloader):
          input_ids = batch['input_ids'].to(device)
          attention_mask = batch['attention_mask'].to(device)
          labels = batch['label'].to(device)

          optimizer.zero_grad()
          outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
          logits = outputs.logits
          loss = outputs.loss
          if i % 100 == 0:
            print(f"epoch {epoch}, batch number {i}, loss {loss}")
          loss.backward()
          optimizer.step()

  # Evaluation
  model.eval()
  all_preds = []
  all_labels = []

  for batch in test_dataloader:
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      labels = batch['label'].to(device)

      with torch.no_grad():
          outputs = model(input_ids, attention_mask=attention_mask)

      logits = outputs.logits
      preds = torch.argmax(logits, dim=1).cpu().numpy()

      all_preds.extend(preds)
      all_labels.extend(labels.cpu().numpy())

  # Convert numerical labels back to text labels
  predicted_labels = [id2label[label] for label in all_preds]
  true_labels = [id2label[label] for label in all_labels]

  # Print classification report
  print(classification_report(true_labels, predicted_labels))
  return model, tokenizer

"""## zero-shot on XLM-RoBERTa-base


"""

train_model('xlm-roberta-base', train_df, test_df, id2label, zero_shot = True)

"""## zero-shot on XLM-RoBERTa-large"""

train_model('xlm-roberta-large', train_df, test_df, id2label, zero_shot = True)

"""## fine_tune XLM-RoBERTa-base

"""

roberta_base_model = train_model('xlm-roberta-base', train_df, test_df, id2label, zero_shot = False)

roberta_base_model.save_pretrained('RoBERTa-base-model')

!huggingface-cli upload 'RoBERTa-base-model'

"""## fine-tune XLM-RoBERTa-large"""

roberta_large_model, roberta_large_tokenizer = train_model('xlm-roberta-large', train_df, test_df, id2label, zero_shot = False)

roberta_large_model.save_pretrained('RoBERTa-large-model')
roberta_large_tokenizer.save_pretrained('RoBERTa-large-model')

!huggingface-cli upload 'RoBERTa-large-model'

"""## zero-shot on ParsBERT"""

train_model('HooshvareLab/bert-base-parsbert-uncased', train_df, test_df, id2label, zero_shot = True)

"""## fine-tune ParsBERT"""

ParseBERT_model = train_model('HooshvareLab/bert-base-parsbert-uncased', train_df, test_df, id2label, zero_shot = False)

ParseBERT_model.save_pretrained('parsBERT-model')

!huggingface-cli login

!huggingface-cli upload 'parsBERT-model'

"""## Improving the best model"""

new_best_model, new_best_tokenizer = train_model('am-azadi/RoBERTa-large-model', train_df, test_df, id2label, zero_shot = False, number_of_epochs = 2, learning_rates = [1e-7, 1e-9])

new_best_model.save_pretrained('best-model')
new_best_tokenizer.save_pretrained('best-model')

!huggingface-cli upload 'best-model'

"""# Evaluation"""

def preprocess(data_path):
  label2id = {'SAD': 0, 'HAPPY': 1, 'HATE': 2, 'SURPRISE': 3, 'ANGRY': 4, 'FEAR': 5, 'OTHER': 6}
  df = pd.read_csv(data_path, sep='\t', header=None)
  df.columns = ['text', 'label']
  df['label'] = train_df['label'].map(label2id)
  return df

class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

def predict_and_evaluate(df):

  tokenizer = AutoTokenizer.from_pretrained('am-azadi/best-model')
  model = AutoModelForSequenceClassification.from_pretrained('am-azadi/best-model', num_labels = 7)
  id2label = {0: 'SAD', 1: 'HAPPY', 2: 'HATE', 3: 'SURPRISE', 4: 'ANGRY', 5: 'FEAR', 6: 'OTHER'}

  # Move the model to GPU
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model.to(device)

  # Create datasets and dataloaders
  dataset = CustomDataset(df['text'].values, df['label'].values, tokenizer)

  dataloader = DataLoader(dataset, batch_size=8, shuffle=False)

  # Evaluation
  model.eval()
  all_preds = []
  all_labels = []

  for batch in dataloader:
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      labels = batch['label'].to(device)

      with torch.no_grad():
          outputs = model(input_ids, attention_mask=attention_mask)

      logits = outputs.logits
      preds = torch.argmax(logits, dim=1).cpu().numpy()

      all_preds.extend(preds)
      all_labels.extend(labels.cpu().numpy())

  # Convert numerical labels back to text labels
  predicted_labels = [id2label[label] for label in all_preds]
  true_labels = [id2label[label] for label in all_labels]

  # Print classification report
  print(classification_report(true_labels, predicted_labels))
  return predicted_labels

"""## some mismatched Sentences"""

predicted_labels = predict_and_evaluate(test_df)

def get_mislabeled_sentences(df, predicted_labels):
  filtered_df = df
  filtered_df['predicted_label'] = predicted_labels
  filtered_df['label'] = filtered_df['label'].map(id2label)
  filtered_df = filtered_df[filtered_df['label'] != predicted_labels]
  return filtered_df

filtered_df = get_mislabeled_sentences(test_df, predicted_labels)
print(len(filtered_df))

filtered_df

