# -*- coding: utf-8 -*-
"""DL-Project-Predict&Evaluate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1--lukAajbM3JC_yDA6spCBueaHR5U92L
"""

import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import pipeline, XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import classification_report

def preprocess(data_path):
  label2id = {'SAD': 0, 'HAPPY': 1, 'HATE': 2, 'SURPRISE': 3, 'ANGRY': 4, 'FEAR': 5, 'OTHER': 6}
  df = pd.read_csv(data_path, sep='\t', header=None)
  df.columns = ['text', 'label']
  df['label'] = df['label'].map(label2id)
  return df

class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

def predict_and_evaluate(df):

  tokenizer = AutoTokenizer.from_pretrained('am-azadi/best-model')
  model = AutoModelForSequenceClassification.from_pretrained('am-azadi/best-model', num_labels = 7)
  id2label = {0: 'SAD', 1: 'HAPPY', 2: 'HATE', 3: 'SURPRISE', 4: 'ANGRY', 5: 'FEAR', 6: 'OTHER'}

  # Move the model to GPU
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model.to(device)

  # Create datasets and dataloaders
  dataset = CustomDataset(df['text'].values, df['label'].values, tokenizer)

  dataloader = DataLoader(dataset, batch_size=8, shuffle=False)

  # Evaluation
  model.eval()
  all_preds = []
  all_labels = []

  for batch in dataloader:
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      labels = batch['label'].to(device)

      with torch.no_grad():
          outputs = model(input_ids, attention_mask=attention_mask)

      logits = outputs.logits
      preds = torch.argmax(logits, dim=1).cpu().numpy()

      all_preds.extend(preds)
      all_labels.extend(labels.cpu().numpy())

  # Convert numerical labels back to text labels
  predicted_labels = [id2label[label] for label in all_preds]
  true_labels = [id2label[label] for label in all_labels]

  # Print classification report
  print(classification_report(true_labels, predicted_labels))
  return predicted_labels

df = preprocess('/content/drive/MyDrive/University/Deep Learning/Project/dataset/test.tsv')

pred = predict_and_evaluate(df)

